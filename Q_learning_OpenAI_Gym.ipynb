{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning OpenAI Gym.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuJzDd6YcYqH5ceVC4kwxS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebastianoscarlopez/learning-deep-learning/blob/master/Q_learning_OpenAI_Gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E3b1zglZRzo",
        "colab_type": "text"
      },
      "source": [
        "# Q-Learning from scratch\n",
        "\n",
        "Reinforce Learning to solve the easy text based games from OpenaAI Gym\n",
        "\n",
        "Choose between Frozen Lake 4x4 or 8x8, Taxi, NChain and Roulette."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzgA-JMYZ5vL",
        "colab_type": "text"
      },
      "source": [
        "# Base Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luUTxO0WVac9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33VnKzUTZ_sa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "cf033a9a-ddaf-4021-f60d-9fff8192c03a"
      },
      "source": [
        "#env = gym.make('FrozenLake-v0')\n",
        "#env = gym.make('FrozenLake8x8-v0')\n",
        "env = gym.make('Taxi-v3')\n",
        "#env = gym.make('NChain-v0')\n",
        "#env = gym.make('Roulette-v0')\n",
        "env._max_episode_steps = 500\n",
        "# The game's look. WARNING NChain-v0 and Roulette-v0 hasn't render\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omn5Cyw5yI4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e999779e-d5f4-4c53-81fb-f0b70f3d293e"
      },
      "source": [
        "# q-table\n",
        "# Rows are every cell on the board (FrozenLake-v0 -> 16 = 4 x 4)\n",
        "# Columns are the possible actions (FrozenLake-v0 -> 4 = Up, Rigth, Down, Left)\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "print('q_table shape', Q.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "q_table shape (500, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmRvg7SYsVd2",
        "colab_type": "text"
      },
      "source": [
        "# Generic code for Q-learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwbWx5Yadc5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defines\n",
        "num_episodes = 20000\n",
        "max_steps_per_episode = env._max_episode_steps\n",
        "# learning rate\n",
        "alpha = 0.1\n",
        "# discount rate\n",
        "gamma = 0.98\n",
        "#Â exploration rate\n",
        "exploration = 1\n",
        "exploration_max = 1\n",
        "exploration_min = 0.01\n",
        "exploration_decay = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VgK-fyZ10Bv",
        "colab_type": "text"
      },
      "source": [
        "Update q_table  \n",
        "\n",
        "$Q(s,a) = (1 - \\alpha) * Q(s,a) + \\alpha * (R_t + \\gamma * \\frac {maxQ} {a'} (s',a'))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7U5UAi5coQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_q_table(s, a, s_new, reward):\n",
        "  Q[s,a] = (1 - alpha) * Q[s,a] + alpha * (reward + gamma * Q[s_new].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uf1AyTltwOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "outputId": "ec3bf61a-cf09-4294-a713-6cc3803352e8"
      },
      "source": [
        "rewards_all_episodes = []\n",
        "total_steps = []\n",
        "for episode in range(num_episodes):\n",
        "    # initialize new episode params\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    reward_episode = 0\n",
        "    for step in range(max_steps_per_episode): \n",
        "      # Exploration-exploitation trade-off\n",
        "      epsilon = np.random.uniform()\n",
        "      # explotation or exploration\n",
        "      action = Q[state].argmax() if epsilon > exploration else env.action_space.sample()\n",
        "\n",
        "      # Take new action\n",
        "      state_new, reward, done, info = env.step(action)\n",
        "      # Update Q-table\n",
        "      update_q_table(state, action, state_new, reward)\n",
        "      #learn(state, state_new, reward, action)\n",
        "      # Set new state\n",
        "      state = state_new\n",
        "      # Add new reward\n",
        "      reward_episode += reward\n",
        "\n",
        "      # did the game end?\n",
        "      if done == True:\n",
        "        break\n",
        "    total_steps.append(step)\n",
        "\n",
        "    # Exploration rate decay\n",
        "    exploration = exploration_min + (exploration_max - exploration_min) * np.exp(-exploration_decay*episode)\n",
        "\n",
        "    # Add current episode reward to total rewards list\n",
        "    rewards_all_episodes.append(reward_episode)\n",
        "\n",
        "    # progress\n",
        "    if episode > 0 and episode % 1000 == 0:\n",
        "      print('episode', episode, 'exploration', exploration, 'avg', np.average(rewards_all_episodes[-1000:]))\n",
        "\n",
        "print('max steps', np.max(total_steps))\n",
        "print(Q)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 1000 exploration 0.3742006467597279 avg -466.156\n",
            "episode 2000 exploration 0.1439819304042466 avg -11.258\n",
            "episode 3000 exploration 0.05928919768418531 avg 3.007\n",
            "episode 4000 exploration 0.028132482499846838 avg 5.75\n",
            "episode 5000 exploration 0.016670567529094613 avg 6.91\n",
            "episode 6000 exploration 0.012453964654899695 avg 7.093\n",
            "episode 7000 exploration 0.010902763145898971 avg 7.591\n",
            "episode 8000 exploration 0.010332108001623487 avg 7.472\n",
            "episode 9000 exploration 0.010122175706045813 avg 7.531\n",
            "episode 10000 exploration 0.010044945930464861 avg 7.278\n",
            "episode 11000 exploration 0.010016534683782344 avg 7.51\n",
            "episode 12000 exploration 0.010006082770229794 avg 7.406\n",
            "episode 13000 exploration 0.010002237726112912 avg 7.353\n",
            "episode 14000 exploration 0.010000823213431913 avg 7.45\n",
            "episode 15000 exploration 0.010000302843297297 avg 7.41\n",
            "episode 16000 exploration 0.010000111409822971 avg 7.199\n",
            "episode 17000 exploration 0.010000040985383415 avg 7.591\n",
            "episode 18000 exploration 0.010000015077679947 avg 7.549\n",
            "episode 19000 exploration 0.010000005546768474 avg 7.594\n",
            "max steps 499\n",
            "[[  0.           0.           0.           0.           0.\n",
            "    0.        ]\n",
            " [  2.51083753   3.22656363  -0.67414484   3.14994782   8.36234335\n",
            "   -5.44846116]\n",
            " [  5.31601605  10.56912701   8.50368862  10.50107374  13.27445578\n",
            "    1.13168478]\n",
            " ...\n",
            " [ -0.52052277   8.96568618  -0.35747883  -0.62754345  -8.7416854\n",
            "   -8.77288753]\n",
            " [ -3.68123785  -2.96462193  -3.26684678   6.17134468 -10.97609012\n",
            "  -11.60439097]\n",
            " [ 13.57467934   6.56527666  15.12047629  18.59999092   5.59369475\n",
            "    6.37999573]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adyGam5Xb0jm",
        "colab_type": "text"
      },
      "source": [
        "# One play\n",
        "\n",
        "With Frozen Lake game bear in mind that it's frozen so don't always move where the agent wants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrzHmEPqbu3F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35f32439-a3bf-4e6c-c588-6bd2cca19bf9"
      },
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "done = False\n",
        "while(not done):\n",
        "  action = Q[state].argmax()\n",
        "  state, reward, done, info = env.step(action)\n",
        "  env.render()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[42mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | :\u001b[42m_\u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}